{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "D9iIjZvZeEEo",
        "E3RnfVUceRM5",
        "akGK5YpRjcu_"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thm1118/Colab_Project/blob/main/Colab_ChatGLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![visitors](https://visitor-badge.glitch.me/badge?page_id=wsh.colab_chatglm)\n",
        "\n",
        "># **基于清华大学发布的对话语言模型ChatGLM**\n",
        "> Colab notebook by [Happy_WSH](https://space.bilibili.com/8417436)。\n",
        "\n",
        ">我做了什么？\n",
        ">\n",
        ">编写此notebook及操作教程；编写了Colab操作风格的WSH方法；测试并给出适合Colab免费用户使用的配置\n",
        "\n",
        "模型作者github：[THUDM/ChatGLM-6B](https://github.com/THUDM/ChatGLM-6B)\n",
        "\n",
        "秋叶方法github：[Akegarasu/ChatGLM-webui](https://github.com/Akegarasu/ChatGLM-webui)\n",
        "\n",
        "WSH方法github：[WSH032/ChatGLM-6](https://github.com/WSH032/ChatGLM-6B)\n",
        "\n",
        "使用时请遵守Apache-2.0 license协议，及各github仓库要求的协议\n",
        "\n",
        "`不过，由于 ChatGLM-6B 的规模较小，目前已知其具有相当多的局限性，如事实性/数学逻辑错误，可能生成有害/有偏见内容，较弱的上下文能力，自我认知混乱，以及对英文指示生成与中文指示完全矛盾的内容。请大家在使用前了解这些问题，以免产生误解。更大的基于 1300 亿参数 GLM-130B 的 ChatGLM 正在内测开发中。---模型作者THUDM`\n"
      ],
      "metadata": {
        "id": "mYraIiRZzU40"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# (一)克隆github，安装依赖，配置环境"
      ],
      "metadata": {
        "id": "BF-6N5YbIxAh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ##*挂载谷歌硬盘（可选）*\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "UjcLVSLBLdHX",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "52QXFL73_0Pl",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title ##1.1克隆秋叶的库、安装依赖\n",
        "\n",
        "!git clone https://github.com/Akegarasu/ChatGLM-webui\n",
        "%cd /content/ChatGLM-webui\n",
        "print(f\"正在安装依赖，请耐心等待\")\n",
        "!pip install --upgrade -r requirements.txt  > /dev/null 2>&1\n",
        "print(f\"依赖安装完成\")\n",
        "\n",
        "#切换编码\n",
        "import locale\n",
        "print(locale.getpreferredencoding())\n",
        "locale.setlocale(locale.LC_ALL, 'en_US.UTF-8')\n",
        "print(locale.getpreferredencoding())\n",
        "import os\n",
        "os.environ['PYTHONIOENCODING'] = 'UTF-8'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## *如果你使用CPU模型，请按照如下教程(可选)*"
      ],
      "metadata": {
        "id": "C_dgVputCfzF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#运行这个代码找到libcuda.so.1文件\n",
        "!sudo find /usr/ -name 'libcuda.so.1'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W1ySfC1XCm0l",
        "outputId": "678af08e-0fed-4043-9a18-88c1ea9d4a99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/cuda-11.8/compat/libcuda.so.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#运行这个代码找到环境路径(被\":\"分割为两个)\n",
        "!echo $LD_LIBRARY_PATH"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13mv1tejDSAw",
        "outputId": "da7f634f-35aa-4a2c-c012-18a3666f5d19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 将cuda文件，两个环境路径填入代码块，并运行\n",
        "cuda_file_path = \"/usr/local/cuda-11.8/compat/libcuda.so.1\" #@param {type:\"string\"}\n",
        "environment_path1 = \"/usr/local/nvidia/lib\" #@param {type:\"string\"}\n",
        "environment_path2 = \"/usr/local/nvidia/lib64\" #@param {type:\"string\"}\n",
        "\n",
        "import os\n",
        "\n",
        "!mkdir -p {environment_path1}\n",
        "!mkdir -p {environment_path2}\n",
        "\n",
        "!cp -p {cuda_file_path} {environment_path1}\n",
        "!cp -p {cuda_file_path} {environment_path2}\n",
        "\n",
        "print(\"cuda文件拷贝完成\")\n",
        "\n",
        "#@markdown **!!!运行完后，点击`代码执行程序`--`重新启动代码执行程序`完成刷新!!!**"
      ],
      "metadata": {
        "cellView": "form",
        "id": "WBmCewXMBVTz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 显示GPU 详细信息\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "metadata": {
        "id": "QzAysDXHTVJy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# （二）选择一个方法并使用"
      ],
      "metadata": {
        "id": "noglVVk7d7xN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1秋叶方法（WebUI风格）`功能多，但gradio在Colab里不稳定`"
      ],
      "metadata": {
        "id": "D9iIjZvZeEEo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ###2.1.1 设置参数并开始对话\n",
        "\n",
        "%cd /content/ChatGLM-webui\n",
        "\n",
        "extArgs=\"\"\n",
        "\n",
        "#@markdown 是否用CPU进行推理`CPU模式请选择chatglm-6b-int4-qe，不然会爆ram`\n",
        "use_cpu = False #@param {type:\"boolean\"}\n",
        "#启用CPU模型\n",
        "if use_cpu:\n",
        "  extArgs = extArgs + \"--cpu \"\n",
        "\n",
        "#@markdown 选择模型`Colab免费用户只能使用int4和qe模型`，或者填入自定义模型路径`将会覆盖预设模型选择`\n",
        "model_path = \"THUDM/chatglm-6b-int4\" #@param [\"THUDM/chatglm-6b\", \"THUDM/chatglm-6b-int4\", \"THUDM/chatglm-6b-int4-qe\"]\n",
        "your_model_path = \"\" #@param {type:\"string\"}\n",
        "#用自定义路径覆盖预设\n",
        "if your_model_path:\n",
        "  model_path = your_model_path\n",
        "\n",
        "#@markdown 推理精度`留空则自动指定, fp32只有CPU可使用 ； int4、int8只有GPU能用`\n",
        "precision = \"\" #@param [\"\", \"fp32\", \"fp16\", \"int4\", \"int8\"]\n",
        "#指定精度\n",
        "if precision:\n",
        "  extArgs = extArgs + f\"--precision={precision} \"\n",
        "\n",
        "#启动\n",
        "!python webui.py --model-path={model_path} --listen --share {extArgs}"
      ],
      "metadata": {
        "cellView": "form",
        "id": "jIiNeWyKAL6V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2WSH方法（Colab风格）`在Colab里兼容较好`"
      ],
      "metadata": {
        "id": "E3RnfVUceRM5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ###2.2.1选择并启用模型\n",
        "\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "import gradio as gr\n",
        "\n",
        "#@markdown 是否用CPU进行推理`CPU模式请选择chatglm-6b-int4-qe，不然会爆ram`\n",
        "use_cpu = False #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown 选择模型`Colab免费用户只能使用int4和qe模型`，或者填入自定义模型路径`将会覆盖预设模型选择`\n",
        "model_path = \"THUDM/chatglm-6b-int4\" #@param [\"THUDM/chatglm-6b\", \"THUDM/chatglm-6b-int4\", \"THUDM/chatglm-6b-int4-qe\"]\n",
        "your_model_path = \"\" #@param {type:\"string\"}\n",
        "#用自定义路径覆盖预设\n",
        "if your_model_path:\n",
        "  model_path = your_model_path\n",
        "\n",
        "if use_cpu:\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
        "  model = AutoModel.from_pretrained(model_path, trust_remote_code=True).float()\n",
        "else:\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
        "  model = AutoModel.from_pretrained(model_path, trust_remote_code=True).half().cuda()\n",
        "\n",
        "model = model.eval()\n",
        "\n",
        "#初始化参数\n",
        "history = []\n",
        "count = 1\n",
        "max_length = 2048\n",
        "top_p = 0.7\n",
        "temperature =  0.95\n",
        "max_turns = 20\n",
        "clear_history_flag = False\n",
        "\n",
        "def clear_history_set():\n",
        "  global history, count, clear_history_flag\n",
        "  history = []\n",
        "  count = 1\n",
        "  clear_history_flag = False"
      ],
      "metadata": {
        "cellView": "form",
        "id": "m_FzCEwuUvKq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ###2.2.2参数设置（对话中也可更改）\n",
        "max_length = 2048 #@param {type:\"number\"}\n",
        "top_p = 0.7 #@param {type:\"slider\", min:0, max:1, step:0.01}\n",
        "temperature =  0.95 #@param {type:\"slider\", min:0, max:1, step:0.01}\n",
        "max_turns = 20 #@param {type:\"slider\", min:1, max:256, step:1}"
      ],
      "metadata": {
        "cellView": "form",
        "id": "pqaSs3MgbnV4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ###2.2.3提问\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "from ipywidgets import Layout\n",
        "\n",
        "#@markdown 提问\n",
        "ask = \"\\u4F60\\u597D\" #@param {type:\"string\"}\n",
        "\n",
        "#清空历史\n",
        "if clear_history_flag:\n",
        "  clear_history_set()\n",
        "  print(f\"达到对话次数上限，历史对话记录已被清空\")\n",
        "\n",
        "print(f\"第 {count} 次对话，到达 {max_turns} 次后，下一次对话时将删除历史对话记录\")\n",
        "\n",
        "#用于根据文本框行数自动调整高度\n",
        "def get_bigger (args):\n",
        "  a = textarea_param.value.count ('\\n') + 1\n",
        "  b = int(len(textarea_param.value) / 100 ) +1\n",
        "  l = max(a,b)\n",
        "  textarea_param.rows = l\n",
        "\n",
        "#设置文本框大小\n",
        "text_layout = Layout (flex='0 1 auto', height='auto', min_height='40px', width='1400px')\n",
        "textarea_param = widgets.Textarea (value='', placeholder='回答...', description='ChatGLM:', disabled=False, layout=text_layout)\n",
        "textarea_param.observe (get_bigger, 'value')\n",
        "display(textarea_param)\n",
        "\n",
        "#通过ipywidgets输出回答内容\n",
        "def ask_and_ans(tokenizer, ask, history, max_length, top_p, temperature):\n",
        "  old_history = \"\"\n",
        "  old_response = \"\"\n",
        "  for response, history in model.stream_chat(tokenizer, ask, history, max_length, top_p, temperature):\n",
        "    textarea_param.value = response\n",
        "    old_response = response\n",
        "    old_history = history\n",
        "  return old_response, old_history\n",
        "\n",
        "\n",
        "#提问及计数+1\n",
        "response, history = ask_and_ans(tokenizer, ask, history, max_length=max_length, top_p=top_p,temperature=temperature)\n",
        "count = count +1\n",
        "\n",
        "#设置清空标志\n",
        "if count > max_turns:\n",
        "  clear_history_flag = True\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "XYEVkeAqduqL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ###2.2.4 历史对话记录\n",
        "\n",
        "\n",
        "\n",
        "#@markdown 显示对话历史\n",
        "show_history = True #@param {type:\"boolean\"}\n",
        "if show_history:\n",
        "  for ask_contet,ans_content in history:\n",
        "    print(f\"用户： {ask_contet}\")\n",
        "    print(f\"回答： {ans_content}\")\n",
        "    print(f\"------------------------------------------------------\")\n",
        "#@markdown 手动清空对话历史\n",
        "clear_history = False #@param {type:\"boolean\"}\n",
        "if clear_history:\n",
        "  clear_history_set()\n",
        "  print(f\"聊天记录已被清空\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "kTAUdZZ1kniH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3官方流式方法"
      ],
      "metadata": {
        "id": "akGK5YpRjcu_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ###2.3.1选择并启用模型\n",
        "\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "import gradio as gr\n",
        "\n",
        "#@markdown 是否用CPU进行推理`CPU模式请选择chatglm-6b-int4-qe，不然会爆ram`\n",
        "use_cpu = False #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown 选择模型`Colab免费用户只能使用int4和qe模型`，或者填入自定义模型路径`将会覆盖预设模型选择`\n",
        "model_path = \"THUDM/chatglm-6b-int4\" #@param [\"THUDM/chatglm-6b\", \"THUDM/chatglm-6b-int4\", \"THUDM/chatglm-6b-int4-qe\"]\n",
        "your_model_path = \"\" #@param {type:\"string\"}\n",
        "#用自定义路径覆盖预设\n",
        "if your_model_path:\n",
        "  model_path = your_model_path\n",
        "\n",
        "if use_cpu:\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
        "  model = AutoModel.from_pretrained(model_path, trust_remote_code=True).float()\n",
        "else:\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
        "  model = AutoModel.from_pretrained(model_path, trust_remote_code=True).half().cuda()\n",
        "\n",
        "model = model.eval()\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "mRlNHn6FmATu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ###2.3.2开启对话\n",
        "\n",
        "#@markdown 最大对话轮数\n",
        "MAX_TURNS = 20 #@param {type:\"slider\", min:1, max:256, step:1}\n",
        "MAX_BOXES = MAX_TURNS * 2\n",
        "\n",
        "\n",
        "\n",
        "def predict(input, max_length, top_p, temperature, history=None):\n",
        "    if history is None:\n",
        "        history = []\n",
        "    for response, history in model.stream_chat(tokenizer, input, history, max_length=max_length, top_p=top_p,\n",
        "                                               temperature=temperature):\n",
        "        updates = []\n",
        "        for query, response in history:\n",
        "            updates.append(gr.update(visible=True, value=\"用户：\" + query))\n",
        "            updates.append(gr.update(visible=True, value=\"ChatGLM-6B：\" + response))\n",
        "        if len(updates) < MAX_BOXES:\n",
        "            updates = updates + [gr.Textbox.update(visible=False)] * (MAX_BOXES - len(updates))\n",
        "        yield [history] + updates\n",
        "\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    state = gr.State([])\n",
        "    text_boxes = []\n",
        "    for i in range(MAX_BOXES):\n",
        "        if i % 2 == 0:\n",
        "            text_boxes.append(gr.Markdown(visible=False, label=\"提问：\"))\n",
        "        else:\n",
        "            text_boxes.append(gr.Markdown(visible=False, label=\"回复：\"))\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=4):\n",
        "            txt = gr.Textbox(show_label=False, placeholder=\"Enter text and press enter\", lines=11).style(\n",
        "                container=False)\n",
        "        with gr.Column(scale=1):\n",
        "            max_length = gr.Slider(0, 4096, value=2048, step=1.0, label=\"Maximum length\", interactive=True)\n",
        "            top_p = gr.Slider(0, 1, value=0.7, step=0.01, label=\"Top P\", interactive=True)\n",
        "            temperature = gr.Slider(0, 1, value=0.95, step=0.01, label=\"Temperature\", interactive=True)\n",
        "            button = gr.Button(\"Generate\")\n",
        "    button.click(predict, [txt, max_length, top_p, temperature, state], [state] + text_boxes)\n",
        "demo.queue().launch()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "nBSLEcXyjoBZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# *（三）开发备用代码*"
      ],
      "metadata": {
        "id": "HSWWiFDvjBn2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "old_response = \"\"\n",
        "for response, history in model.stream_chat(tokenizer, \"你好\", [], max_length=2048, top_p=0.7, temperature=0.95):\n",
        "  print(response[len(old_response):], end=\"\")\n",
        "  old_response = response\n",
        "print(end=\"\\r\")\n",
        "print(old_response)"
      ],
      "metadata": {
        "id": "5jh610GcQe5m",
        "outputId": "14bb8813-cea7-46af-be74-313a5023ac17",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "你好👋！我是人工智能助手 ChatGLM-6B，很高兴见到你，欢迎问我任何问题。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "old_response = \"\"\n",
        "for response, history in model.stream_chat(tokenizer, \"你好\", [], max_length=2048, top_p=0.7, temperature=0.95):\n",
        "  old_response = response\n",
        "  sys.stdout.write(response)\n",
        "  sys.stdout.flush()\n",
        "  sys.stdout.write(\"\\r\")\n",
        "print(old_response) #"
      ],
      "metadata": {
        "id": "Rtju7dXTRc9E",
        "outputId": "bbdee23d-00b5-481d-dd28-15875f8bdd38",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "你好👋！我是人工智能助手 ChatGLM-6B，很高兴见到你，欢迎问我任何问题。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def ask_and_ans(tokenizer, ask, history, max_length, top_p, temperature):\n",
        "  old_history = \"\"\n",
        "  old_response = \"\"\n",
        "  for response, history in model.stream_chat(tokenizer, ask, history, max_length, top_p, temperature):\n",
        "    old_response = response\n",
        "    old_history = history\n",
        "    print(end=\"\\r\")\n",
        "    print(response, end=\"\", flush=True) # 打印当前字符串\n",
        "  print(end=\"\\r\")\n",
        "  print(old_response)\n",
        "  return old_response, old_history"
      ],
      "metadata": {
        "id": "-r0JZsMKB-aR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import output\n",
        "def ask_and_ans(tokenizer, ask, history, max_length, top_p, temperature):\n",
        "  old_history = \"\"\n",
        "  old_response = \"\"\n",
        "  for response, history in model.stream_chat(tokenizer, ask, history, max_length, top_p, temperature):\n",
        "    print(response[len(old_response):], end=\"\")\n",
        "    old_response = response\n",
        "    old_history = history\n",
        "  output.clear()\n",
        "  print(old_response)\n",
        "  return old_response, old_history"
      ],
      "metadata": {
        "id": "2EE60jIPB-bL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ###终止按钮\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "from ipywidgets import Layout\n",
        "import threading\n",
        "\n",
        "ask = \"\\u6211\\u7231\\u4F60\" #@param {type:\"string\"}\n",
        "\n",
        "#用于终止ask_and_ans\n",
        "stop_flag = False\n",
        "\n",
        "#清空历史\n",
        "if clear_history_flag:\n",
        "  clear_history_set()\n",
        "  print(f\"达到对话次数上限，历史对话记录已被清空\")\n",
        "\n",
        "print(f\"第 {count} 次对话，到达 {max_turns} 次后，下一次对话时将删除历史对话记录\")\n",
        "\n",
        "\n",
        "# 定义一个回调函数，它会在按钮被点击时调用\n",
        "def stop_button_clicked(b):\n",
        "    # 使用global关键字，声明要修改全局变量stop_flag的值\n",
        "    global stop_flag\n",
        "    # 将stop_flag的值设为True，表示要终止子线程\n",
        "    print(\"终止回答.\")\n",
        "    stop_flag = True\n",
        "\n",
        "# 创建一个按钮控件，并为它注册回调函数\n",
        "stop_button = widgets.Button(description=\"终止回答\")\n",
        "stop_button.on_click(stop_button_clicked)\n",
        "# 显示按钮控件\n",
        "display(stop_button)\n",
        "\n",
        "\n",
        "#用于根据文本框行数自动调整高度\n",
        "def get_bigger (args):\n",
        "  textarea_param.rows = textarea_param.value.count ('\\n') + 1\n",
        "\n",
        "#设置文本框大小\n",
        "text_layout = Layout (flex='0 1 auto', height='auto', min_height='100px', width='auto')\n",
        "textarea_param = widgets.Textarea (value='', placeholder='回答...', description='ChatGLM:', disabled=False, layout=text_layout)\n",
        "textarea_param.observe (get_bigger, 'value')\n",
        "display(textarea_param)\n",
        "\n",
        "#通过ipywidgets输出回答内容\n",
        "def ask_and_ans(tokenizer, ask, history, max_length, top_p, temperature):\n",
        "  global stop_flag\n",
        "  old_history = \"\"\n",
        "  old_response = \"\"\n",
        "  for response, history in model.stream_chat(tokenizer, ask, history, max_length, top_p, temperature):\n",
        "    textarea_param.value = response\n",
        "    old_response = response\n",
        "    old_history = history\n",
        "    if stop_flag:\n",
        "      # 获取当前线程实例\n",
        "      thread = threading.current_thread()\n",
        "      # 将返回值赋给result属性\n",
        "      thread.result = (old_response, old_history)\n",
        "      return old_response, old_history\n",
        "  # 获取当前线程实例\n",
        "  thread = threading.current_thread()\n",
        "  # 将返回值赋给result属性\n",
        "  thread.result = (old_response, old_history)\n",
        "  return old_response, old_history\n",
        "\n",
        "# 创建一个子线程，并将ask_and_ans作为目标函数\n",
        "stop_thread = threading.Thread(target=ask_and_ans, args=(tokenizer, ask, history, max_length, top_p, temperature))\n",
        "# 启动子线程\n",
        "stop_thread.start()\n",
        "# 等待子线程结束\n",
        "stop_thread.join()\n",
        "# 获取子线程的返回值\n",
        "response, history = stop_thread.result\n",
        "count = count +1\n",
        "\n",
        "#设置清空标志\n",
        "if count > max_turns:\n",
        "  clear_history_flag = True\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "CNygeoOO_dfA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}